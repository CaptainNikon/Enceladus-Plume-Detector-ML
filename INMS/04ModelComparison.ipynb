{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f22843c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jerem\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:14: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.4)\n",
      "  from scipy.sparse import csr_matrix, issparse\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../configs'))\n",
    "from config import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26097d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load CSV without parsing dates\n",
    "df = pd.read_csv('CompiledDataSet/INMS_V1.csv')\n",
    "\n",
    "# Step 2: Convert the date column using the correct format\n",
    "df['TIME'] = pd.to_datetime(df['sclk'], format='%Y-%jT%H:%M:%S.%f', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08a0c72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using training subset: 241247 rows\n"
     ]
    }
   ],
   "source": [
    "use_training_subset = True #Use only 10min interval around the plume\n",
    "\n",
    "if use_training_subset:\n",
    "    df_ml = df[df['TRAINING_SUBSET'] == True].copy()\n",
    "    print(f\"Using training subset: {df_ml.shape[0]} rows\")\n",
    "else:\n",
    "    df_ml = df.copy()\n",
    "    print(f\"Using full data: {df_ml.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f160e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "class ModelBenchmark:\n",
    "    def __init__(self, models, param_grids, df, feature_names, feature_cols, target_col, sample_frac):\n",
    "        \"\"\"\n",
    "        models: dict of form {\"rf\": RandomForestClassifier(), \"lr\": LogisticRegression(), ...}\n",
    "        param_grids: dict of form {\"rf\": {...}, \"lr\": {...}}\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "        X_test, y_test: Test data\n",
    "        feature_names: list of feature names\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.param_grids = param_grids\n",
    "        self.df = df\n",
    "\n",
    "        # Sample fraction for fast testing\n",
    "        if sample_frac < 1.0:\n",
    "            df_ml = df.sample(frac=sample_frac, random_state=42).copy()\n",
    "        else:\n",
    "            df_ml = df.copy()\n",
    "        X = df_ml[feature_cols]\n",
    "        y = df_ml[target_col]\n",
    "\n",
    "        # Split for model comparison (as above)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.feature_names = feature_names\n",
    "        self.results = []\n",
    "\n",
    "        # BEST MODEL TRACKING\n",
    "        self.best_score = -float('inf')\n",
    "        self.best_model = None\n",
    "        self.best_params = None\n",
    "        self.best_name = None\n",
    "\n",
    "    def hypertune(self, model_key, n_iter=20, scoring='f1', cv=5):\n",
    "        model = self.models[model_key]\n",
    "        param_grid = self.param_grids.get(model_key, {})\n",
    "        searcher = RandomizedSearchCV(\n",
    "            model,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=n_iter,      \n",
    "            scoring=scoring,\n",
    "            cv=cv,             \n",
    "            verbose=0,          \n",
    "            n_jobs=10,      \n",
    "            random_state=42\n",
    "        )\n",
    "        searcher.fit(self.X_train, self.y_train)\n",
    "        return searcher.best_estimator_, searcher.best_params_, searcher.best_score_\n",
    "\n",
    "\n",
    "    def evaluate(self, clf, mdl_name):\n",
    "        y_test_pred = clf.predict(self.X_test)\n",
    "        y_test_prob = clf.predict_proba(self.X_test)[:, 1]\n",
    "        test_report = classification_report(self.y_test, y_test_pred, digits=3, output_dict=True)\n",
    "        \n",
    "        # Access feature importances only if estimator supports it\n",
    "        feature_importances = None\n",
    "        # For pipeline with XGBoost\n",
    "        if hasattr(clf, \"named_steps\") and \"xgb\" in clf.named_steps:\n",
    "            feature_importances = clf.named_steps[\"xgb\"].feature_importances_\n",
    "        elif hasattr(clf, \"feature_importances_\"):\n",
    "            feature_importances = clf.feature_importances_\n",
    "        \n",
    "        result = {\n",
    "            \"model\": mdl_name,\n",
    "            \"roc_auc_test\": roc_auc_score(self.y_test, y_test_prob),\n",
    "            \"test_report\": test_report,\n",
    "            \"conf_matrix_test\": confusion_matrix(self.y_test, y_test_pred),\n",
    "            \"feature_importances\": feature_importances\n",
    "        }\n",
    "        self.results.append(result)\n",
    "        print(f\"\\n===== {mdl_name} Results =====\")\n",
    "        print(\"Test:\")\n",
    "        print(classification_report(self.y_test, y_test_pred, digits=3))\n",
    "        if feature_importances is not None:\n",
    "            print(\"Feature Importances:\")\n",
    "            feat_imp_df = pd.DataFrame({\n",
    "                \"Feature\": self.feature_names,\n",
    "                \"Importance\": feature_importances\n",
    "            }).sort_values(\"Importance\", ascending=False)\n",
    "            print(feat_imp_df)\n",
    "\n",
    "\n",
    "    def run_all(self):\n",
    "        for key in self.models:\n",
    "            print(f\"\\nHypertuning {key}...\")\n",
    "            best_clf, best_params, best_score = self.hypertune(key)\n",
    "            print(\"Best Params:\", best_params)\n",
    "            print(f\"Best CV Score: {best_score:.3f}\")\n",
    "            self.evaluate(best_clf, key)\n",
    "            \n",
    "            # Update best if this model is better\n",
    "            if best_score > self.best_score:\n",
    "                self.best_score = best_score\n",
    "                self.best_model = best_clf\n",
    "                self.best_params = best_params\n",
    "                self.best_name = key\n",
    "\n",
    "    def summary(self):\n",
    "        df_sum = pd.DataFrame([\n",
    "            {\n",
    "                \"model\": r[\"model\"],\n",
    "                \"roc_auc_test\": r[\"roc_auc_test\"],\n",
    "                \"test_f1\": r[\"test_report\"][\"1\"][\"f1-score\"]\n",
    "            } for r in self.results\n",
    "        ])\n",
    "        print(\"\\nModel Comparison Summary:\")\n",
    "        print(df_sum.sort_values(\"roc_auc_test\", ascending=False))\n",
    "        \n",
    "        # EXTRA: Show Best Model Details\n",
    "        print(\"\\n=== Best Model Overall ===\")\n",
    "        print(f\"Model: {self.best_name}\")\n",
    "        print(f\"Best CV Score: {self.best_score:.3f}\")\n",
    "        print(f\"Best Params: {self.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6a1bca4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypertuning xg...\n",
      "Best Params: {'xgb__subsample': 0.8, 'xgb__scale_pos_weight': 1, 'xgb__n_estimators': 100, 'xgb__min_child_weight': 1, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.2, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.8}\n",
      "Best CV Score: 0.997\n",
      "\n",
      "===== xg Results =====\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000     69225\n",
      "           1      0.994     0.998     0.996      3150\n",
      "\n",
      "    accuracy                          1.000     72375\n",
      "   macro avg      0.997     0.999     0.998     72375\n",
      "weighted avg      1.000     1.000     1.000     72375\n",
      "\n",
      "Feature Importances:\n",
      "            Feature  Importance\n",
      "9        sc_pos_t_y    0.598685\n",
      "6      view_dir_t_x    0.107074\n",
      "7      view_dir_t_y    0.077200\n",
      "5             alt_t    0.066374\n",
      "1        targ_pos_y    0.062414\n",
      "12         c1counts    0.027550\n",
      "0        targ_pos_x    0.019871\n",
      "10       sc_pos_t_z    0.018923\n",
      "3     velocity_comp    0.006739\n",
      "8      view_dir_t_z    0.005617\n",
      "11       sc_vel_t_x    0.005572\n",
      "2        targ_pos_z    0.003275\n",
      "4   mass_per_charge    0.000707\n",
      "13         c2counts    0.000000\n",
      "\n",
      "Model Comparison Summary:\n",
      "  model  roc_auc_test   test_f1\n",
      "0    xg      0.999996  0.996041\n",
      "\n",
      "=== Best Model Overall ===\n",
      "Model: xg\n",
      "Best CV Score: 0.997\n",
      "Best Params: {'xgb__subsample': 0.8, 'xgb__scale_pos_weight': 1, 'xgb__n_estimators': 100, 'xgb__min_child_weight': 1, 'xgb__max_depth': 6, 'xgb__learning_rate': 0.2, 'xgb__gamma': 0.1, 'xgb__colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "from models.randomforest import get_model as get_rf, param_grid as rf_grid\n",
    "from models.logisticregression import get_model as get_lr, param_grid as lr_grid\n",
    "from models.xgboost import get_model as get_xg, param_grid as xg_grid\n",
    "from models.knn import get_model as get_knn, param_grid as knn_grid\n",
    "\n",
    "models = {\n",
    "    #\"rf\": get_rf(),\n",
    "    #\"lr\": get_lr(),\n",
    "    \"xg\": get_xg(),\n",
    "    #\"knn\": get_knn(),\n",
    "}\n",
    "param_grids = {\n",
    "    #\"rf\": rf_grid,\n",
    "    #\"lr\": lr_grid,\n",
    "    \"xg\": xg_grid,\n",
    "    #\"knn\": knn_grid\n",
    "}\n",
    "\n",
    "# Define features and target for ML (drop unnecessary columns)\n",
    "exclude_cols = ['sclk', 'source', 'TIME', 'FLYBY', 'TRAINING_SUBSET',\"TIME_FROM_CA\"] # \"alt_t\"\n",
    "feature_cols = [col for col in df_ml.columns if col not in exclude_cols + ['PLUME']]\n",
    "#feature_cols = [\"c1counts\", \"c2counts\", \"mass_per_charge\",\"velocity_comp\" ]\n",
    "target_col = 'PLUME'\n",
    "\n",
    "benchmark = ModelBenchmark(models, param_grids, df_ml, feature_cols, feature_cols, \"PLUME\", sample_frac=1)\n",
    "\n",
    "benchmark.run_all()\n",
    "benchmark.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0e58d4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using full data: 1418837 rows\n"
     ]
    }
   ],
   "source": [
    "use_training_subset = False #Use only 10min interval around the plume\n",
    "\n",
    "if use_training_subset:\n",
    "    df_ml = df[df['TRAINING_SUBSET'] == True].copy()\n",
    "    print(f\"Using training subset: {df_ml.shape[0]} rows\")\n",
    "else:\n",
    "    df_ml = df.copy()\n",
    "    print(f\"Using full data: {df_ml.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Logistic Regression on full data...\n",
      "Logistic Regression CV F1 scores: [0.90282277 0.90699285 0.91491815 0.91810144 0.91435185]\n",
      "Logistic Regression Mean F1: 0.911 (+/- 0.006)\n",
      "Logistic Regression Total elapsed time: 9.0 seconds\n",
      "\n",
      "Running Random Forest on full data...\n",
      "Random Forest CV F1 scores: [0.99976196 0.99952381 0.99976196 0.99856938 0.99976196]\n",
      "Random Forest Mean F1: 0.999 (+/- 0.000)\n",
      "Random Forest Total elapsed time: 130.0 seconds\n",
      "\n",
      "Running KNN on full data...\n",
      "KNN CV F1 scores: [0.88536585 0.87853659 0.89396887 0.88274174 0.88597768]\n",
      "KNN Mean F1: 0.885 (+/- 0.005)\n",
      "KNN Total elapsed time: 12.5 seconds\n",
      "\n",
      "Running XGBoost on full data...\n",
      "XGBoost CV F1 scores: [0.97366548 0.973659   0.98261491 0.97788462 0.97253403]\n",
      "XGBoost Mean F1: 0.976 (+/- 0.004)\n",
      "XGBoost Total elapsed time: 15.8 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Model best hyperparameters from your results\n",
    "params_lr = {\n",
    "    \"solver\": \"newton-cholesky\",\n",
    "    \"penalty\": \"l2\",\n",
    "    \"max_iter\": 2000,\n",
    "    \"class_weight\": None,\n",
    "    \"C\": 100.0\n",
    "}\n",
    "params_rf = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"max_features\": \"sqrt\",\n",
    "    \"max_depth\": 8,\n",
    "    \"class_weight\": None\n",
    "}\n",
    "params_knn = {\n",
    "    \"n_neighbors\": 9,\n",
    "    \"weights\": \"distance\",\n",
    "    \"metric\": \"manhattan\",\n",
    "    \"leaf_size\": 50,\n",
    "    \"algorithm\": \"auto\"\n",
    "}\n",
    "params_xgb = {\n",
    "    \"subsample\": 0.8,\n",
    "    \"scale_pos_weight\": 1,\n",
    "    \"n_estimators\": 100,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.2,\n",
    "    \"gamma\": 0.1,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"random_state\": 42,\n",
    "    \"use_label_encoder\": False,\n",
    "    \"eval_metric\": \"logloss\"\n",
    "}\n",
    "\n",
    "\n",
    "# Feature and target extraction\n",
    "X = df_ml[feature_cols]\n",
    "y = df_ml[target_col]\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, pipeline in [\n",
    "    (\"Logistic Regression\", Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(**params_lr))\n",
    "    ])),\n",
    "    (\"Random Forest\", Pipeline([\n",
    "        (\"clf\", RandomForestClassifier(**params_rf))\n",
    "    ])),\n",
    "    (\"KNN\", Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", KNeighborsClassifier(**params_knn))\n",
    "    ])),\n",
    "    (\"XGBoost\", Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", XGBClassifier(**params_xgb))\n",
    "    ]))\n",
    "]:\n",
    "    print(f\"\\nRunning {name} on full data...\")\n",
    "    start = time.time()\n",
    "    scores = cross_val_score(\n",
    "        pipeline, X, y, cv=cv, scoring='f1', n_jobs=-1\n",
    "    )\n",
    "    comp_time = time.time() - start\n",
    "    print(f\"{name} CV F1 scores: {scores}\")\n",
    "    print(f\"{name} Mean F1: {np.mean(scores):.3f} (+/- {np.std(scores):.3f})\")\n",
    "    print(f\"{name} Total computation time: {comp_time:.1f} seconds\")\n",
    "    results[name] = {'scores': scores, 'mean': np.mean(scores), 'std': np.std(scores), 'time': comp_time}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6051f42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
